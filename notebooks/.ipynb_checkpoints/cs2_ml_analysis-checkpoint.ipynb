{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CS2 Demo ML Analysis Notebook\n",
    "\n",
    "This notebook demonstrates machine learning analysis on real CS2 demo data using the fps-genie system.\n",
    "\n",
    "## Available Demo Files:\n",
    "- `vitality-vs-spirit-m1-dust2.dem` - Professional match data\n",
    "- `test_demo.dem` - Test demo file\n",
    "- `minimal_demo.bin` - Minimal demo for quick testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# Set up plotting\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Parse Demo Data\n",
    "\n",
    "First, let's parse a demo file using our CS2 demo parser to extract structured data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define paths\n",
    "demo_file = \"/workspace/test_data/vitality-vs-spirit-m1-dust2.dem\"\n",
    "output_dir = \"/workspace/notebooks/parsed_data\"\n",
    "\n",
    "# Create output directory\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "print(f\"Parsing demo file: {demo_file}\")\n",
    "print(f\"Output directory: {output_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse the demo using our CS2 demo analyzer\n",
    "try:\n",
    "    result = subprocess.run([\n",
    "        \"/usr/local/bin/cs2-demo-analyzer\",\n",
    "        \"--input\", demo_file,\n",
    "        \"--output\", f\"{output_dir}/parsed_data.json\",\n",
    "        \"--format\", \"json\"\n",
    "    ], capture_output=True, text=True, timeout=300)\n",
    "    \n",
    "    if result.returncode == 0:\n",
    "        print(\"âœ… Demo parsing successful!\")\n",
    "        print(f\"Output: {result.stdout}\")\n",
    "    else:\n",
    "        print(f\"âŒ Demo parsing failed: {result.stderr}\")\n",
    "        print(f\"Return code: {result.returncode}\")\n",
    "        \n",
    "except subprocess.TimeoutExpired:\n",
    "    print(\"â° Demo parsing timed out after 5 minutes\")\n",
    "except Exception as e:\n",
    "    print(f\"âŒ Error running demo analyzer: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load and Explore Parsed Data\n",
    "\n",
    "Load the parsed demo data and explore its structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load parsed data\n",
    "parsed_data_file = f\"{output_dir}/parsed_data.json\"\n",
    "\n",
    "if os.path.exists(parsed_data_file):\n",
    "    with open(parsed_data_file, 'r') as f:\n",
    "        demo_data = json.load(f)\n",
    "    \n",
    "    print(\"ðŸ“Š Demo Data Structure:\")\n",
    "    for key in demo_data.keys():\n",
    "        if isinstance(demo_data[key], list):\n",
    "            print(f\"  {key}: {len(demo_data[key])} items\")\n",
    "        else:\n",
    "            print(f\"  {key}: {type(demo_data[key])}\")\nelse:\n",
    "    print(\"âŒ No parsed data file found. Demo parsing may have failed.\")\n",
    "    # Create sample data for demonstration\n",
    "    demo_data = {\n",
    "        \"rounds\": [],\n",
    "        \"kills\": [],\n",
    "        \"players\": [],\n",
    "        \"match_info\": {}\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Convert to DataFrames for Analysis\n",
    "\n",
    "Convert the parsed data into pandas DataFrames for easier analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to DataFrames\n",
    "if 'kills' in demo_data and demo_data['kills']:\n",
    "    kills_df = pd.DataFrame(demo_data['kills'])\n",
    "    print(f\"ðŸ“ˆ Kills DataFrame: {len(kills_df)} records\")\n",
    "    print(kills_df.head())\nelse:\n",
    "    print(\"No kills data available\")\n",
    "    kills_df = pd.DataFrame()\n",
    "\n",
    "if 'rounds' in demo_data and demo_data['rounds']:\n",
    "    rounds_df = pd.DataFrame(demo_data['rounds'])\n",
    "    print(f\"\\nðŸŽ¯ Rounds DataFrame: {len(rounds_df)} records\")\n",
    "    print(rounds_df.head())\nelse:\n",
    "    print(\"No rounds data available\")\n",
    "    rounds_df = pd.DataFrame()\n",
    "\n",
    "if 'players' in demo_data and demo_data['players']:\n",
    "    players_df = pd.DataFrame(demo_data['players'])\n",
    "    print(f\"\\nðŸ‘¥ Players DataFrame: {len(players_df)} records\")\n",
    "    print(players_df.head())\nelse:\n",
    "    print(\"No players data available\")\n",
    "    players_df = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Basic Statistical Analysis\n",
    "\n",
    "Perform basic statistical analysis on the demo data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic statistics\n",
    "if not kills_df.empty:\n",
    "    print(\"ðŸ”« Kill Statistics:\")\n",
    "    print(f\"  Total kills: {len(kills_df)}\")\n",
    "    \n",
    "    if 'weapon' in kills_df.columns:\n",
    "        weapon_counts = kills_df['weapon'].value_counts()\n",
    "        print(f\"  Most used weapon: {weapon_counts.index[0]} ({weapon_counts.iloc[0]} kills)\")\n",
    "    \n",
    "    if 'attacker' in kills_df.columns:\n",
    "        top_fragger = kills_df['attacker'].value_counts().head(1)\n",
    "        if not top_fragger.empty:\n",
    "            print(f\"  Top fragger: {top_fragger.index[0]} ({top_fragger.iloc[0]} kills)\")\n",
    "\nif not rounds_df.empty:\n",
    "    print(f\"\\nðŸŽ¯ Round Statistics:\")\n",
    "    print(f\"  Total rounds: {len(rounds_df)}\")\n",
    "    \n",
    "    if 'winner' in rounds_df.columns:\n",
    "        team_wins = rounds_df['winner'].value_counts()\n",
    "        print(f\"  Team wins: {dict(team_wins)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Data Visualization\n",
    "\n",
    "Create visualizations of the CS2 demo data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create visualizations\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "fig.suptitle('CS2 Demo Analysis Dashboard', fontsize=16, fontweight='bold')\n",
    "\n",
    "# Plot 1: Weapon usage (if available)\n",
    "if not kills_df.empty and 'weapon' in kills_df.columns:\n",
    "    weapon_counts = kills_df['weapon'].value_counts().head(10)\n",
    "    axes[0, 0].bar(range(len(weapon_counts)), weapon_counts.values)\n",
    "    axes[0, 0].set_xticks(range(len(weapon_counts)))\n",
    "    axes[0, 0].set_xticklabels(weapon_counts.index, rotation=45, ha='right')\n",
    "    axes[0, 0].set_title('Top 10 Weapons by Kills')\n",
    "    axes[0, 0].set_ylabel('Number of Kills')\nelse:\n",
    "    axes[0, 0].text(0.5, 0.5, 'No weapon data available', ha='center', va='center', transform=axes[0, 0].transAxes)\n",
    "    axes[0, 0].set_title('Weapon Usage')\n",
    "\n",
    "# Plot 2: Round winners (if available)\n",
    "if not rounds_df.empty and 'winner' in rounds_df.columns:\n",
    "    team_wins = rounds_df['winner'].value_counts()\n",
    "    axes[0, 1].pie(team_wins.values, labels=team_wins.index, autopct='%1.1f%%')\n",
    "    axes[0, 1].set_title('Round Wins by Team')\nelse:\n",
    "    axes[0, 1].text(0.5, 0.5, 'No round data available', ha='center', va='center', transform=axes[0, 1].transAxes)\n",
    "    axes[0, 1].set_title('Round Wins')\n",
    "\n",
    "# Plot 3: Kills per player (if available)\n",
    "if not kills_df.empty and 'attacker' in kills_df.columns:\n",
    "    player_kills = kills_df['attacker'].value_counts().head(10)\n",
    "    axes[1, 0].barh(range(len(player_kills)), player_kills.values)\n",
    "    axes[1, 0].set_yticks(range(len(player_kills)))\n",
    "    axes[1, 0].set_yticklabels(player_kills.index)\n",
    "    axes[1, 0].set_title('Top 10 Players by Kills')\n",
    "    axes[1, 0].set_xlabel('Number of Kills')\nelse:\n",
    "    axes[1, 0].text(0.5, 0.5, 'No player kill data available', ha='center', va='center', transform=axes[1, 0].transAxes)\n",
    "    axes[1, 0].set_title('Player Kills')\n",
    "\n",
    "# Plot 4: Sample time series or summary\n",
    "if not rounds_df.empty:\n",
    "    axes[1, 1].plot(range(len(rounds_df)), [1] * len(rounds_df), 'o-')\n",
    "    axes[1, 1].set_title('Round Progression')\n",
    "    axes[1, 1].set_xlabel('Round Number')\n",
    "    axes[1, 1].set_ylabel('Game State')\nelse:\n",
    "    axes[1, 1].text(0.5, 0.5, 'No temporal data available', ha='center', va='center', transform=axes[1, 1].transAxes)\n",
    "    axes[1, 1].set_title('Time Series')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Machine Learning Pipeline Test\n",
    "\n",
    "Test the ML pipeline with the parsed demo data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test ML pipeline\n",
    "print(\"ðŸ¤– Testing ML Pipeline...\")\n",
    "\n",
    "try:\n",
    "    # Run the ML pipeline on our demo data\n",
    "    result = subprocess.run([\n",
    "        \"/usr/local/bin/cs2-ml\",\n",
    "        \"--input\", demo_file,\n",
    "        \"--mode\", \"analyze\"\n",
    "    ], capture_output=True, text=True, timeout=120)\n",
    "    \n",
    "    if result.returncode == 0:\n",
    "        print(\"âœ… ML Pipeline successful!\")\n",
    "        print(f\"Output: {result.stdout}\")\n",
    "    else:\n",
    "        print(f\"âŒ ML Pipeline failed: {result.stderr}\")\n",
    "        print(f\"Return code: {result.returncode}\")\n",
    "        \n",
    "except subprocess.TimeoutExpired:\n",
    "    print(\"â° ML Pipeline timed out after 2 minutes\")\n",
    "except Exception as e:\n",
    "    print(f\"âŒ Error running ML pipeline: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Feature Engineering for ML\n",
    "\n",
    "Create features from the demo data for machine learning models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature engineering example\n",
    "if not kills_df.empty:\n",
    "    print(\"ðŸ”§ Engineering features from demo data...\")\n",
    "    \n",
    "    # Example features\n",
    "    features = {}\n",
    "    \n",
    "    if 'attacker' in kills_df.columns:\n",
    "        # Player performance features\n",
    "        player_stats = kills_df.groupby('attacker').agg({\n",
    "            'attacker': 'count',  # total kills\n",
    "        }).rename(columns={'attacker': 'total_kills'})\n",
    "        \n",
    "        if 'headshot' in kills_df.columns:\n",
    "            headshot_rate = kills_df.groupby('attacker')['headshot'].mean()\n",
    "            player_stats['headshot_rate'] = headshot_rate\n",
    "        \n",
    "        print(f\"Created features for {len(player_stats)} players\")\n",
    "        print(player_stats.head())\n",
    "        \n",
    "        # Save features for ML\n",
    "        player_stats.to_csv(f\"{output_dir}/player_features.csv\")\n",
    "        print(f\"ðŸ’¾ Features saved to {output_dir}/player_features.csv\")\n",
    "    \nelse:\n",
    "    print(\"No data available for feature engineering\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Real-time Analysis Simulation\n",
    "\n",
    "Simulate real-time analysis capabilities on the demo data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate real-time analysis\n",
    "print(\"âš¡ Simulating real-time analysis...\")\n",
    "\n",
    "if not rounds_df.empty:\n",
    "    # Simulate processing rounds one by one\n",
    "    cumulative_stats = []\n",
    "    \n",
    "    for i in range(min(10, len(rounds_df))):\n",
    "        round_subset = rounds_df.iloc[:i+1]\n",
    "        \n",
    "        stats = {\n",
    "            'round': i + 1,\n",
    "            'total_rounds': len(round_subset)\n",
    "        }\n",
    "        \n",
    "        if 'winner' in round_subset.columns:\n",
    "            team_wins = round_subset['winner'].value_counts()\n",
    "            for team, wins in team_wins.items():\n",
    "                stats[f'{team}_wins'] = wins\n",
    "        \n",
    "        cumulative_stats.append(stats)\n",
    "    \n",
    "    # Convert to DataFrame and display\n",
    "    realtime_df = pd.DataFrame(cumulative_stats)\n",
    "    print(\"ðŸ“ˆ Real-time Statistics Evolution:\")\n",
    "    print(realtime_df)\n",
    "    \n",
    "    # Plot real-time progression\n",
    "    if len(realtime_df) > 1:\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        \n",
    "        for col in realtime_df.columns:\n",
    "            if col.endswith('_wins'):\n",
    "                plt.plot(realtime_df['round'], realtime_df[col], marker='o', label=col)\n",
    "        \n",
    "        plt.title('Real-time Match Progression')\n",
    "        plt.xlabel('Round Number')\n",
    "        plt.ylabel('Wins')\n",
    "        plt.legend()\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        plt.show()\nelse:\n",
    "    print(\"No round data available for real-time simulation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Performance Metrics\n",
    "\n",
    "Calculate and display performance metrics for the analysis pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performance metrics\n",
    "import time\n",
    "\n",
    "print(\"ðŸ“Š Performance Metrics:\")\n",
    "\n",
    "# Measure data processing performance\n",
    "start_time = time.time()\n",
    "\n",
    "# Simulate some data processing\n",
    "if not kills_df.empty:\n",
    "    # Complex aggregation\n",
    "    complex_stats = kills_df.groupby(['attacker']).agg({\n",
    "        'attacker': 'count'\n",
    "    })\n",
    "    \n",
    "    processing_time = time.time() - start_time\n",
    "    \n",
    "    print(f\"  Data processing time: {processing_time:.4f} seconds\")\n",
    "    print(f\"  Records processed: {len(kills_df)}\")\n",
    "    print(f\"  Processing rate: {len(kills_df)/processing_time:.2f} records/second\")\nelse:\n",
    "    print(\"  No data available for performance testing\")\n",
    "\n",
    "# Memory usage\n",
    "import psutil\n",
    "import os\n",
    "\n",
    "process = psutil.Process(os.getpid())\n",
    "memory_info = process.memory_info()\n",
    "print(f\"  Memory usage: {memory_info.rss / 1024 / 1024:.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Next Steps and Recommendations\n",
    "\n",
    "Based on the analysis, here are recommendations for further development."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ðŸŽ¯ Analysis Summary and Recommendations:\")\nprint(\"=\"*50)\n",
    "\n",
    "if not demo_data or not any(demo_data.values()):\n",
    "    print(\"â— No demo data was successfully parsed.\")\n",
    "    print(\"\\nðŸ“‹ Immediate Action Items:\")\n",
    "    print(\"  1. Check demo file format compatibility\")\n",
    "    print(\"  2. Verify cs2-demo-analyzer is working correctly\")\n",
    "    print(\"  3. Test with different demo files\")\n",
    "    print(\"  4. Check parser output logs for errors\")\nelse:\n",
    "    print(\"âœ… Demo data successfully loaded and analyzed!\")\n",
    "    \n",
    "    data_quality = 0\n",
    "    if not kills_df.empty:\n",
    "        data_quality += 1\n",
    "        print(f\"  âœ… Kill events: {len(kills_df)} records\")\n",
    "    if not rounds_df.empty:\n",
    "        data_quality += 1\n",
    "        print(f\"  âœ… Round data: {len(rounds_df)} records\")\n",
    "    if not players_df.empty:\n",
    "        data_quality += 1\n",
    "        print(f\"  âœ… Player data: {len(players_df)} records\")\n",
    "    \n",
    "    print(f\"\\nðŸ“ˆ Data Quality Score: {data_quality}/3\")\n",
    "    \n",
    "    print(\"\\nðŸš€ Next Development Steps:\")\n",
    "    if data_quality >= 2:\n",
    "        print(\"  1. Implement advanced ML models (player ranking, outcome prediction)\")\n",
    "        print(\"  2. Add real-time streaming analysis\")\n",
    "        print(\"  3. Create interactive dashboards\")\n",
    "        print(\"  4. Implement team strategy analysis\")\n",
    "    else:\n",
    "        print(\"  1. Improve data parsing and extraction\")\n",
    "        print(\"  2. Add more demo file format support\")\n",
    "        print(\"  3. Enhance data validation\")\n",
    "\n",
    "print(\"\\nðŸ”§ Technical Recommendations:\")\nprint(\"  1. Set up automated demo processing pipeline\")\nprint(\"  2. Implement data validation and quality checks\")\nprint(\"  3. Add performance monitoring and optimization\")\nprint(\"  4. Create ML model training and evaluation framework\")\n",
    "\n",
    "print(\"\\nðŸ“š This notebook demonstrates:\")\nprint(\"  â€¢ Demo file parsing and data extraction\")\nprint(\"  â€¢ Statistical analysis of CS2 gameplay data\")\nprint(\"  â€¢ Data visualization and insights\")\nprint(\"  â€¢ ML pipeline integration\")\nprint(\"  â€¢ Real-time analysis simulation\")\nprint(\"  â€¢ Performance monitoring\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

