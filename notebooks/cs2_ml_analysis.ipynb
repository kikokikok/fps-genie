{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CS2 Demo ML Analysis Notebook\n",
    "\n",
    "This notebook demonstrates machine learning analysis on real CS2 demo data using the fps-genie system.\n",
    "\n",
    "## Available Demo Files:\n",
    "- `vitality-vs-spirit-m1-dust2.dem` - Professional match data\n",
    "- `test_demo.dem` - Test demo file\n",
    "- `minimal_demo.bin` - Minimal demo for quick testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# Set up plotting\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Parse Demo Data\n",
    "\n",
    "First, let's parse a demo file using our CS2 demo parser to extract structured data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define paths\n",
    "demo_file = \"/workspace/test_data/vitality-vs-spirit-m1-dust2.dem\"\n",
    "output_dir = \"/workspace/notebooks/parsed_data\"\n",
    "\n",
    "# Create output directory\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "print(f\"Parsing demo file: {demo_file}\")\n",
    "print(f\"Output directory: {output_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse the demo using our CS2 demo analyzer\n",
    "try:\n",
    "    result = subprocess.run([\n",
    "        \"/usr/local/bin/cs2-demo-analyzer\",\n",
    "        \"--input\", demo_file,\n",
    "        \"--output\", f\"{output_dir}/parsed_data.json\",\n",
    "        \"--format\", \"json\"\n",
    "    ], capture_output=True, text=True, timeout=300)\n",
    "    \n",
    "    if result.returncode == 0:\n",
    "        print(\"‚úÖ Demo parsing successful!\")\n",
    "        print(f\"Output: {result.stdout}\")\n",
    "    else:\n",
    "        print(f\"‚ùå Demo parsing failed: {result.stderr}\")\n",
    "        print(f\"Return code: {result.returncode}\")\n",
    "        \n",
    "except subprocess.TimeoutExpired:\n",
    "    print(\"‚è∞ Demo parsing timed out after 5 minutes\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error running demo analyzer: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load and Explore Parsed Data\n",
    "\n",
    "Load the parsed demo data and explore its structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load parsed data\n",
    "parsed_data_file = f\"{output_dir}/parsed_data.json\"\n",
    "\n",
    "if os.path.exists(parsed_data_file):\n",
    "    with open(parsed_data_file, 'r') as f:\n",
    "        demo_data = json.load(f)\n",
    "    \n",
    "    print(\"üìä Demo Data Structure:\")\n",
    "    for key in demo_data.keys():\n",
    "        if isinstance(demo_data[key], list):\n",
    "            print(f\"  {key}: {len(demo_data[key])} items\")\n",
    "        else:\n",
    "            print(f\"  {key}: {type(demo_data[key])}\")\nelse:\n",
    "    print(\"‚ùå No parsed data file found. Demo parsing may have failed.\")\n",
    "    # Create sample data for demonstration\n",
    "    demo_data = {\n",
    "        \"rounds\": [],\n",
    "        \"kills\": [],\n",
    "        \"players\": [],\n",
    "        \"match_info\": {}\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Convert to DataFrames for Analysis\n",
    "\n",
    "Convert the parsed data into pandas DataFrames for easier analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to DataFrames\n",
    "if 'kills' in demo_data and demo_data['kills']:\n",
    "    kills_df = pd.DataFrame(demo_data['kills'])\n",
    "    print(f\"üìà Kills DataFrame: {len(kills_df)} records\")\n",
    "    print(kills_df.head())\nelse:\n",
    "    print(\"No kills data available\")\n",
    "    kills_df = pd.DataFrame()\n",
    "\n",
    "if 'rounds' in demo_data and demo_data['rounds']:\n",
    "    rounds_df = pd.DataFrame(demo_data['rounds'])\n",
    "    print(f\"\\nüéØ Rounds DataFrame: {len(rounds_df)} records\")\n",
    "    print(rounds_df.head())\nelse:\n",
    "    print(\"No rounds data available\")\n",
    "    rounds_df = pd.DataFrame()\n",
    "\n",
    "if 'players' in demo_data and demo_data['players']:\n",
    "    players_df = pd.DataFrame(demo_data['players'])\n",
    "    print(f\"\\nüë• Players DataFrame: {len(players_df)} records\")\n",
    "    print(players_df.head())\nelse:\n",
    "    print(\"No players data available\")\n",
    "    players_df = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Basic Statistical Analysis\n",
    "\n",
    "Perform basic statistical analysis on the demo data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic statistics\n",
    "if not kills_df.empty:\n",
    "    print(\"üî´ Kill Statistics:\")\n",
    "    print(f\"  Total kills: {len(kills_df)}\")\n",
    "    \n",
    "    if 'weapon' in kills_df.columns:\n",
    "        weapon_counts = kills_df['weapon'].value_counts()\n",
    "        print(f\"  Most used weapon: {weapon_counts.index[0]} ({weapon_counts.iloc[0]} kills)\")\n",
    "    \n",
    "    if 'attacker' in kills_df.columns:\n",
    "        top_fragger = kills_df['attacker'].value_counts().head(1)\n",
    "        if not top_fragger.empty:\n",
    "            print(f\"  Top fragger: {top_fragger.index[0]} ({top_fragger.iloc[0]} kills)\")\n",
    "\nif not rounds_df.empty:\n",
    "    print(f\"\\nüéØ Round Statistics:\")\n",
    "    print(f\"  Total rounds: {len(rounds_df)}\")\n",
    "    \n",
    "    if 'winner' in rounds_df.columns:\n",
    "        team_wins = rounds_df['winner'].value_counts()\n",
    "        print(f\"  Team wins: {dict(team_wins)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Data Visualization\n",
    "\n",
    "Create visualizations of the CS2 demo data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create visualizations\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "fig.suptitle('CS2 Demo Analysis Dashboard', fontsize=16, fontweight='bold')\n",
    "\n",
    "# Plot 1: Weapon usage (if available)\n",
    "if not kills_df.empty and 'weapon' in kills_df.columns:\n",
    "    weapon_counts = kills_df['weapon'].value_counts().head(10)\n",
    "    axes[0, 0].bar(range(len(weapon_counts)), weapon_counts.values)\n",
    "    axes[0, 0].set_xticks(range(len(weapon_counts)))\n",
    "    axes[0, 0].set_xticklabels(weapon_counts.index, rotation=45, ha='right')\n",
    "    axes[0, 0].set_title('Top 10 Weapons by Kills')\n",
    "    axes[0, 0].set_ylabel('Number of Kills')\nelse:\n",
    "    axes[0, 0].text(0.5, 0.5, 'No weapon data available', ha='center', va='center', transform=axes[0, 0].transAxes)\n",
    "    axes[0, 0].set_title('Weapon Usage')\n",
    "\n",
    "# Plot 2: Round winners (if available)\n",
    "if not rounds_df.empty and 'winner' in rounds_df.columns:\n",
    "    team_wins = rounds_df['winner'].value_counts()\n",
    "    axes[0, 1].pie(team_wins.values, labels=team_wins.index, autopct='%1.1f%%')\n",
    "    axes[0, 1].set_title('Round Wins by Team')\nelse:\n",
    "    axes[0, 1].text(0.5, 0.5, 'No round data available', ha='center', va='center', transform=axes[0, 1].transAxes)\n",
    "    axes[0, 1].set_title('Round Wins')\n",
    "\n",
    "# Plot 3: Kills per player (if available)\n",
    "if not kills_df.empty and 'attacker' in kills_df.columns:\n",
    "    player_kills = kills_df['attacker'].value_counts().head(10)\n",
    "    axes[1, 0].barh(range(len(player_kills)), player_kills.values)\n",
    "    axes[1, 0].set_yticks(range(len(player_kills)))\n",
    "    axes[1, 0].set_yticklabels(player_kills.index)\n",
    "    axes[1, 0].set_title('Top 10 Players by Kills')\n",
    "    axes[1, 0].set_xlabel('Number of Kills')\nelse:\n",
    "    axes[1, 0].text(0.5, 0.5, 'No player kill data available', ha='center', va='center', transform=axes[1, 0].transAxes)\n",
    "    axes[1, 0].set_title('Player Kills')\n",
    "\n",
    "# Plot 4: Sample time series or summary\n",
    "if not rounds_df.empty:\n",
    "    axes[1, 1].plot(range(len(rounds_df)), [1] * len(rounds_df), 'o-')\n",
    "    axes[1, 1].set_title('Round Progression')\n",
    "    axes[1, 1].set_xlabel('Round Number')\n",
    "    axes[1, 1].set_ylabel('Game State')\nelse:\n",
    "    axes[1, 1].text(0.5, 0.5, 'No temporal data available', ha='center', va='center', transform=axes[1, 1].transAxes)\n",
    "    axes[1, 1].set_title('Time Series')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Machine Learning Pipeline Test\n",
    "\n",
    "Test the ML pipeline with the parsed demo data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test ML pipeline\n",
    "print(\"ü§ñ Testing ML Pipeline...\")\n",
    "\n",
    "try:\n",
    "    # Run the ML pipeline on our demo data\n",
    "    result = subprocess.run([\n",
    "        \"/usr/local/bin/cs2-ml\",\n",
    "        \"--input\", demo_file,\n",
    "        \"--mode\", \"analyze\"\n",
    "    ], capture_output=True, text=True, timeout=120)\n",
    "    \n",
    "    if result.returncode == 0:\n",
    "        print(\"‚úÖ ML Pipeline successful!\")\n",
    "        print(f\"Output: {result.stdout}\")\n",
    "    else:\n",
    "        print(f\"‚ùå ML Pipeline failed: {result.stderr}\")\n",
    "        print(f\"Return code: {result.returncode}\")\n",
    "        \n",
    "except subprocess.TimeoutExpired:\n",
    "    print(\"‚è∞ ML Pipeline timed out after 2 minutes\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error running ML pipeline: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Feature Engineering for ML\n",
    "\n",
    "Create features from the demo data for machine learning models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature engineering example\n",
    "if not kills_df.empty:\n",
    "    print(\"üîß Engineering features from demo data...\")\n",
    "    \n",
    "    # Example features\n",
    "    features = {}\n",
    "    \n",
    "    if 'attacker' in kills_df.columns:\n",
    "        # Player performance features\n",
    "        player_stats = kills_df.groupby('attacker').agg({\n",
    "            'attacker': 'count',  # total kills\n",
    "        }).rename(columns={'attacker': 'total_kills'})\n",
    "        \n",
    "        if 'headshot' in kills_df.columns:\n",
    "            headshot_rate = kills_df.groupby('attacker')['headshot'].mean()\n",
    "            player_stats['headshot_rate'] = headshot_rate\n",
    "        \n",
    "        print(f\"Created features for {len(player_stats)} players\")\n",
    "        print(player_stats.head())\n",
    "        \n",
    "        # Save features for ML\n",
    "        player_stats.to_csv(f\"{output_dir}/player_features.csv\")\n",
    "        print(f\"üíæ Features saved to {output_dir}/player_features.csv\")\n",
    "    \nelse:\n",
    "    print(\"No data available for feature engineering\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Real-time Analysis Simulation\n",
    "\n",
    "Simulate real-time analysis capabilities on the demo data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate real-time analysis\n",
    "print(\"‚ö° Simulating real-time analysis...\")\n",
    "\n",
    "if not rounds_df.empty:\n",
    "    # Simulate processing rounds one by one\n",
    "    cumulative_stats = []\n",
    "    \n",
    "    for i in range(min(10, len(rounds_df))):\n",
    "        round_subset = rounds_df.iloc[:i+1]\n",
    "        \n",
    "        stats = {\n",
    "            'round': i + 1,\n",
    "            'total_rounds': len(round_subset)\n",
    "        }\n",
    "        \n",
    "        if 'winner' in round_subset.columns:\n",
    "            team_wins = round_subset['winner'].value_counts()\n",
    "            for team, wins in team_wins.items():\n",
    "                stats[f'{team}_wins'] = wins\n",
    "        \n",
    "        cumulative_stats.append(stats)\n",
    "    \n",
    "    # Convert to DataFrame and display\n",
    "    realtime_df = pd.DataFrame(cumulative_stats)\n",
    "    print(\"üìà Real-time Statistics Evolution:\")\n",
    "    print(realtime_df)\n",
    "    \n",
    "    # Plot real-time progression\n",
    "    if len(realtime_df) > 1:\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        \n",
    "        for col in realtime_df.columns:\n",
    "            if col.endswith('_wins'):\n",
    "                plt.plot(realtime_df['round'], realtime_df[col], marker='o', label=col)\n",
    "        \n",
    "        plt.title('Real-time Match Progression')\n",
    "        plt.xlabel('Round Number')\n",
    "        plt.ylabel('Wins')\n",
    "        plt.legend()\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        plt.show()\nelse:\n",
    "    print(\"No round data available for real-time simulation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Performance Metrics\n",
    "\n",
    "Calculate and display performance metrics for the analysis pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performance metrics\n",
    "import time\n",
    "\n",
    "print(\"üìä Performance Metrics:\")\n",
    "\n",
    "# Measure data processing performance\n",
    "start_time = time.time()\n",
    "\n",
    "# Simulate some data processing\n",
    "if not kills_df.empty:\n",
    "    # Complex aggregation\n",
    "    complex_stats = kills_df.groupby(['attacker']).agg({\n",
    "        'attacker': 'count'\n",
    "    })\n",
    "    \n",
    "    processing_time = time.time() - start_time\n",
    "    \n",
    "    print(f\"  Data processing time: {processing_time:.4f} seconds\")\n",
    "    print(f\"  Records processed: {len(kills_df)}\")\n",
    "    print(f\"  Processing rate: {len(kills_df)/processing_time:.2f} records/second\")\nelse:\n",
    "    print(\"  No data available for performance testing\")\n",
    "\n",
    "# Memory usage\n",
    "import psutil\n",
    "import os\n",
    "\n",
    "process = psutil.Process(os.getpid())\n",
    "memory_info = process.memory_info()\n",
    "print(f\"  Memory usage: {memory_info.rss / 1024 / 1024:.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Next Steps and Recommendations\n",
    "\n",
    "Based on the analysis, here are recommendations for further development."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üéØ Analysis Summary and Recommendations:\")\nprint(\"=\"*50)\n",
    "\n",
    "if not demo_data or not any(demo_data.values()):\n",
    "    print(\"‚ùó No demo data was successfully parsed.\")\n",
    "    print(\"\\nüìã Immediate Action Items:\")\n",
    "    print(\"  1. Check demo file format compatibility\")\n",
    "    print(\"  2. Verify cs2-demo-analyzer is working correctly\")\n",
    "    print(\"  3. Test with different demo files\")\n",
    "    print(\"  4. Check parser output logs for errors\")\nelse:\n",
    "    print(\"‚úÖ Demo data successfully loaded and analyzed!\")\n",
    "    \n",
    "    data_quality = 0\n",
    "    if not kills_df.empty:\n",
    "        data_quality += 1\n",
    "        print(f\"  ‚úÖ Kill events: {len(kills_df)} records\")\n",
    "    if not rounds_df.empty:\n",
    "        data_quality += 1\n",
    "        print(f\"  ‚úÖ Round data: {len(rounds_df)} records\")\n",
    "    if not players_df.empty:\n",
    "        data_quality += 1\n",
    "        print(f\"  ‚úÖ Player data: {len(players_df)} records\")\n",
    "    \n",
    "    print(f\"\\nüìà Data Quality Score: {data_quality}/3\")\n",
    "    \n",
    "    print(\"\\nüöÄ Next Development Steps:\")\n",
    "    if data_quality >= 2:\n",
    "        print(\"  1. Implement advanced ML models (player ranking, outcome prediction)\")\n",
    "        print(\"  2. Add real-time streaming analysis\")\n",
    "        print(\"  3. Create interactive dashboards\")\n",
    "        print(\"  4. Implement team strategy analysis\")\n",
    "    else:\n",
    "        print(\"  1. Improve data parsing and extraction\")\n",
    "        print(\"  2. Add more demo file format support\")\n",
    "        print(\"  3. Enhance data validation\")\n",
    "\n",
    "print(\"\\nüîß Technical Recommendations:\")\nprint(\"  1. Set up automated demo processing pipeline\")\nprint(\"  2. Implement data validation and quality checks\")\nprint(\"  3. Add performance monitoring and optimization\")\nprint(\"  4. Create ML model training and evaluation framework\")\n",
    "\n",
    "print(\"\\nüìö This notebook demonstrates:\")\nprint(\"  ‚Ä¢ Demo file parsing and data extraction\")\nprint(\"  ‚Ä¢ Statistical analysis of CS2 gameplay data\")\nprint(\"  ‚Ä¢ Data visualization and insights\")\nprint(\"  ‚Ä¢ ML pipeline integration\")\nprint(\"  ‚Ä¢ Real-time analysis simulation\")\nprint(\"  ‚Ä¢ Performance monitoring\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

